{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TECIX_TrabalhoFinal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOEbBC/8ALAE50XLlHbjeMh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FredDsR/TrabalhoFinal_ML/blob/main/TECIX_TrabalhoFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z_JE0wvvMjk"
      },
      "source": [
        "# Implementação de um classificador de tendência à pensamentos suicidas sobre dados da OMS\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mco9zzBGN6cd"
      },
      "source": [
        "## Introdução\n",
        "Neste notebook se encontra a implementação da etapa de transformação dos dados e a implementação de 3 (três) modelos de aprendizado de máquina supervisionado para predição de trendência ao pensamento suicida em jovens. Os modelos implementados foram *K-Nearest Neighbors (KNN)*, *Support-vector Machine (SVM)* e *Extreme Gradient Boosting (XGBoost)*.\n",
        "\n",
        "### Aquisição dos dados\n",
        "\n",
        "Os dados utilizados nesta implementação foram retirados do estudo Global School-Based Student Health Survey ralizado na Argentina em 2018. Maiores descrições e acesso aos dados estão disponíveis [aqui](https://extranet.who.int/ncdsmicrodata/index.php/catalog/866/study-description)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyshCc3Oz3y0"
      },
      "source": [
        "---\n",
        "\n",
        "### Download e atualização de libs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tpCTMWzNZYC",
        "outputId": "c0c52fba-e805-4cee-faea-a02788370aa1"
      },
      "source": [
        "!wget -O /content/data.csv https://extranet.who.int/ncdsmicrodata/index.php/catalog/866/download/6149\n",
        "\n",
        "!pip install scikit-learn --upgrade\n",
        "!pip install xgboost --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-08 13:59:07--  https://extranet.who.int/ncdsmicrodata/index.php/catalog/866/download/6149\n",
            "Resolving extranet.who.int (extranet.who.int)... 158.232.12.136\n",
            "Connecting to extranet.who.int (extranet.who.int)|158.232.12.136|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14705501 (14M) [application/octet-stream]\n",
            "Saving to: ‘/content/data.csv’\n",
            "\n",
            "/content/data.csv   100%[===================>]  14.02M  3.68MB/s    in 3.8s    \n",
            "\n",
            "2021-11-08 13:59:12 (3.68 MB/s) - ‘/content/data.csv’ saved [14705501/14705501]\n",
            "\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Collecting xgboost\n",
            "  Using cached xgboost-1.5.0-py3-none-manylinux2014_x86_64.whl (173.5 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNhXZoJKzyA1"
      },
      "source": [
        "---\n",
        "\n",
        "### Bibliotecas e ferramentas utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd7y84tKAF8W"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelBinarizer, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSOeXpc70BCM"
      },
      "source": [
        "---\n",
        "\n",
        "### Leitura"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "8WOYNzNdRFR7",
        "outputId": "1a2555b1-bf76-4adb-9e6e-fa85a8589e89"
      },
      "source": [
        "data_df = pd.read_csv('/content/data.csv')\n",
        "\n",
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>site</th>\n",
              "      <th>record</th>\n",
              "      <th>q1</th>\n",
              "      <th>q2</th>\n",
              "      <th>q3</th>\n",
              "      <th>q4</th>\n",
              "      <th>q5</th>\n",
              "      <th>q6</th>\n",
              "      <th>q10</th>\n",
              "      <th>q15</th>\n",
              "      <th>q16</th>\n",
              "      <th>q17</th>\n",
              "      <th>q18</th>\n",
              "      <th>q19</th>\n",
              "      <th>q22</th>\n",
              "      <th>q23</th>\n",
              "      <th>q24</th>\n",
              "      <th>q25</th>\n",
              "      <th>q26</th>\n",
              "      <th>q27</th>\n",
              "      <th>q28</th>\n",
              "      <th>q29</th>\n",
              "      <th>q30</th>\n",
              "      <th>q31</th>\n",
              "      <th>q32</th>\n",
              "      <th>q33</th>\n",
              "      <th>q34</th>\n",
              "      <th>q35</th>\n",
              "      <th>q36</th>\n",
              "      <th>q37</th>\n",
              "      <th>q38</th>\n",
              "      <th>q39</th>\n",
              "      <th>q40</th>\n",
              "      <th>q41</th>\n",
              "      <th>q42</th>\n",
              "      <th>q43</th>\n",
              "      <th>q44</th>\n",
              "      <th>q45</th>\n",
              "      <th>q46</th>\n",
              "      <th>q47</th>\n",
              "      <th>...</th>\n",
              "      <th>qn42</th>\n",
              "      <th>qn43</th>\n",
              "      <th>qn44</th>\n",
              "      <th>qn45</th>\n",
              "      <th>qn46</th>\n",
              "      <th>qn47</th>\n",
              "      <th>qn48</th>\n",
              "      <th>qn49</th>\n",
              "      <th>qn50</th>\n",
              "      <th>qn51</th>\n",
              "      <th>qn52</th>\n",
              "      <th>qn53</th>\n",
              "      <th>qn54</th>\n",
              "      <th>qn55</th>\n",
              "      <th>qn56</th>\n",
              "      <th>qn57</th>\n",
              "      <th>qn58</th>\n",
              "      <th>qn61</th>\n",
              "      <th>qn62</th>\n",
              "      <th>qn63</th>\n",
              "      <th>qn66</th>\n",
              "      <th>qn67</th>\n",
              "      <th>qn68</th>\n",
              "      <th>qnunwtg</th>\n",
              "      <th>qnowtg</th>\n",
              "      <th>qnobeseg</th>\n",
              "      <th>qnff1g</th>\n",
              "      <th>qnff2g</th>\n",
              "      <th>qnff3g</th>\n",
              "      <th>qntob2g</th>\n",
              "      <th>qnnotb2g</th>\n",
              "      <th>qnbcanyg</th>\n",
              "      <th>qnc1g</th>\n",
              "      <th>qnpa5g</th>\n",
              "      <th>qnpa7g</th>\n",
              "      <th>qnpe3g</th>\n",
              "      <th>qnpe5g</th>\n",
              "      <th>weight</th>\n",
              "      <th>stratum</th>\n",
              "      <th>psu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AG</td>\n",
              "      <td>1</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>457.0940</td>\n",
              "      <td>201801010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AG</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>457.0940</td>\n",
              "      <td>201801010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AG</td>\n",
              "      <td>3</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.67</td>\n",
              "      <td>56.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>261.4465</td>\n",
              "      <td>201801010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AG</td>\n",
              "      <td>4</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.73</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>550.9036</td>\n",
              "      <td>201801010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AG</td>\n",
              "      <td>5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>457.0940</td>\n",
              "      <td>201801010</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 124 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  site  record   q1   q2   q3  ...  qnpe3g  qnpe5g    weight    stratum  psu\n",
              "0   AG       1  3.0  2.0  2.0  ...     2.0     2.0  457.0940  201801010   19\n",
              "1   AG       2  3.0  2.0  2.0  ...     2.0     2.0  457.0940  201801010   19\n",
              "2   AG       3  4.0  1.0  4.0  ...     2.0     2.0  261.4465  201801010   19\n",
              "3   AG       4  6.0  1.0  2.0  ...     1.0     1.0  550.9036  201801010   19\n",
              "4   AG       5  3.0  2.0  2.0  ...     2.0     2.0  457.0940  201801010   19\n",
              "\n",
              "[5 rows x 124 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1k7OPyMFoM2"
      },
      "source": [
        "## Limpeza dos dados\n",
        "\n",
        "Além dos dados obtidos através de um questionário auto aplicado, estão presentes atributos não relevantes para o treinamento do modelo, como o atributo `record` (id do participante), o atributo `site` que possui o único valor *AG*, entre outras. Além desses atributos não relevantes, há atributos derivados dos dados coletados pela aplicação do questionário, sendo assim, para redução de dimensão, esses atributos serão removidos.\n",
        "\n",
        "O padrão para atributos do questinário é \"q\" seguido de um número, utilizando esse padrão em um regex é possivél filtrar todas colunas válidas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYIXpU-_Qd31",
        "outputId": "830a5a92-1840-4a43-98a8-c8808e17b0cc"
      },
      "source": [
        "valid_columns = [column for column in data_df.columns if re.match(r'q[1-9]', column)]\n",
        "data_df = data_df[valid_columns]\n",
        "print(\"Colunas válidas:\", list(data_df.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas válidas: ['q1', 'q2', 'q3', 'q4', 'q5', 'q6', 'q10', 'q15', 'q16', 'q17', 'q18', 'q19', 'q22', 'q23', 'q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q30', 'q31', 'q32', 'q33', 'q34', 'q35', 'q36', 'q37', 'q38', 'q39', 'q40', 'q41', 'q42', 'q43', 'q44', 'q45', 'q46', 'q47', 'q48', 'q49', 'q50', 'q51', 'q52', 'q53', 'q54', 'q55', 'q56', 'q57', 'q58', 'q61', 'q62', 'q63', 'q66', 'q67', 'q68']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfRYi2Xy0Zs9"
      },
      "source": [
        "Para o problema proposto, é necessário analisar com atenção a coluna `q24` que contém a resposta da pergunta: *Durante os últimos 12 meses, você alguma vez considerou seriamente a tentativa de suicídio?*\n",
        "\n",
        "Primeiro vamos analisar a porcentagem de dados nulos e a distribuição das classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "xnIW9NuzMU5K",
        "outputId": "9c0e91d4-a870-42f4-f785-4f81262423ad"
      },
      "source": [
        "q24 = data_df['q24'].copy().fillna(3)\n",
        "data_label = {1: 'Sim', 2: 'Não', 3: 'NA'}\n",
        "q24 = q24.map(data_label)\n",
        "ax = sns.histplot(q24)\n",
        "ax.text(2.7, 2, f'NA: {round(100*sum(data_df[\"q24\"].isna())/len(data_df), 2)}%')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jCMKUqFMUsL"
      },
      "source": [
        "Como observado acima, há uma proporção maior da classe \"Não\", porém, acredito que essa característica pertence ao mundo real, logo, isso agrega à generalização do modelo. Outro ponto é que dados nulos correspondem à 2,37% dos exemplos, como é uma fatia pequena, esses exemplos serão removidos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W4ycLoPOb4a",
        "outputId": "be19b875-9b96-4e8c-e571-106906489343"
      },
      "source": [
        "data_df = data_df.dropna(subset = ['q24']).reset_index().drop(['index'], axis=1)\n",
        "data_df['q24'].unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ESnWdK-WWT"
      },
      "source": [
        "Além do atributo `q24`, os demais também possuem entradas nulas, o ideal seria trata-las individualmente, porém, para otimização do tempo, todas serão preenchidas a seguir com a classe majoritária. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPw4nHgH-qHi"
      },
      "source": [
        "for column in data_df.columns:\n",
        "    if not column == 'q24':\n",
        "        mode = data_df[column].mode()[0]\n",
        "        data_df[column].fillna(mode, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN2HAHuOAabF"
      },
      "source": [
        "## Transformação dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xj0EvIHLDKZ"
      },
      "source": [
        "Agora, podemos seguir a transformação dos atributos. Todos os atributos já são númericos, facilitando a transformação dos dados, porém, todos iniciam contagem a partir do \"1\" invéz de \"0\" e perguntas de Sim e Não foram mantidas como 1 e 2, respectivamente. Além disso, há casos de atributos categóricos ordinais bagunçados, então, uma série de funções instanciadas abaixo serão aplicadas ao dataset para transformação dos atributos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYy89kFK0Ms"
      },
      "source": [
        "def transform_binary_columns(dataframe, columns):\n",
        "    for column in columns:\n",
        "        if 2.0 in dataframe[column].unique():\n",
        "            new_values = [0 if value == 2.0 else 1 for value in dataframe[column]]\n",
        "            dataframe[column] = new_values\n",
        "    return dataframe\n",
        "\n",
        "def transform_ordinal_columns(dataframe, columns):\n",
        "    for column in columns:\n",
        "        if 0 not in dataframe[column].unique():\n",
        "            dataframe[column] = dataframe[column].transform(lambda value: int(value - 1))\n",
        "    return dataframe\n",
        "\n",
        "def binarize_categorical_columns(dataframe, columns):\n",
        "    for column in columns:\n",
        "        lb = LabelBinarizer()\n",
        "        lb.fit(dataframe[column])\n",
        "        new_values = lb.transform(dataframe[column])\n",
        "        new_values_df = pd.DataFrame(new_values)\n",
        "        new_values_df.columns = [f'{column}_{str(value).split(\".\")[0]}' for value in lb.classes_]\n",
        "        dataframe = pd.concat([dataframe, new_values_df], axis=1).drop([column], axis=1)\n",
        "    return dataframe\n",
        "\n",
        "def transform_special_ordinal_columns(dataframe, columns):\n",
        "    for column in columns:\n",
        "        unique_values = dataframe[column].unique().tolist()\n",
        "        unique_values.sort()\n",
        "        first = unique_values[0]\n",
        "        others = unique_values[1:]\n",
        "        others = others[::-1]\n",
        "        new_values = [first] + others\n",
        "        relabel_dict = {old_value: int(new_value) for old_value, new_value in zip(unique_values, new_values)}\n",
        "        dataframe[column] = dataframe[column].map(relabel_dict)\n",
        "    return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb9B7heJBkhH"
      },
      "source": [
        "---\n",
        "\n",
        "### Atributos descartados:\n",
        "Há atibutos não relevantes ou que viriam por atrapalhar a generalização do modelo. Abaixo segue uma breve explicação da mitivação da exclusão dessas colunas:\n",
        "\n",
        "- `q19`: é um atributo muito desbalanceado, com 70% dos exemplos agrupados em uma classse e o resto dividido entre as demais. Além de corresponder à causa de uma lesão sofrida pelo participante, o que não agrega substancialmente à generalização do modelo.\n",
        "\n",
        "- `q25` e `q26`: Correspondem, respectivamente, à \"*Fez um plano de suicídio?*\" e \"*Tentativa de suicídio.*\", ambas questões se relacionam diretamente ao problema central sobre tendÊncia ao pensamento suicida, o que dificultaria a generalização do modelo sobre os outros atributos.\n",
        "\n",
        "- `q30`: O atributo foi removido pois 91,9% dos exemplos estão agrupados em uma única classe, tornando o atributo pouco relevante ara generalização."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ1LhImMB09e"
      },
      "source": [
        "useless_columns = ['q19', 'q25', 'q26', 'q30']\n",
        "\n",
        "data_df = data_df.drop(useless_columns, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHrqFT15A2qd"
      },
      "source": [
        "---\n",
        "\n",
        "### Atributos binários:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRrhBCXMA8j6"
      },
      "source": [
        "binary_columns = ['q2', 'q24', 'q44', 'q66', 'q67', 'q68']\n",
        "\n",
        "data_df = transform_binary_columns(data_df, binary_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGo3vyRgJGzX"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Atributos categóricos não ordinais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Op30LfJK02"
      },
      "source": [
        "categorical_columns = ['q18', 'q37', 'q47', 'q48']\n",
        "\n",
        "data_df = binarize_categorical_columns(data_df, categorical_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4BPtGMmG9b-"
      },
      "source": [
        "---\n",
        "\n",
        "### Atributos categóricos ordinais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXTL2oLmHKAS"
      },
      "source": [
        "ordinal_columns = ['q1', 'q3', 'q6', 'q10', 'q15',\n",
        "                   'q16', 'q17', 'q22', 'q23', 'q27',\n",
        "                   'q29', 'q31', 'q32', 'q35', 'q36', \n",
        "                   'q38', 'q39', 'q41', 'q42', 'q43', \n",
        "                   'q46', 'q49', 'q50', 'q51', 'q52', \n",
        "                   'q53', 'q54', 'q55', 'q56', 'q57', \n",
        "                   'q58', 'q61', 'q62', 'q63']\n",
        "\n",
        "data_df = transform_ordinal_columns(data_df, ordinal_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcxVXpi9LNKM"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Atributos categóricos ordinais especiais:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suasss8RLTKr"
      },
      "source": [
        "special_ordinal_columns = ['q28', 'q34', 'q40', 'q45']\n",
        "\n",
        "data_df = transform_special_ordinal_columns(data_df, special_ordinal_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-z0AUmOCIat"
      },
      "source": [
        "---\n",
        "\n",
        "### Tratamento especial atributo q33:\n",
        "\n",
        "O atributo possui categorias divididas entre “Nenhum”, “Pai ou guardião”, “Mãe ou guardiã”, “Os dois” e “Não sei”, para simplificação dos dados, as classes “Pai ou guardião” e “Mãe ou guardiã” serão agrupadas em uma única classe “Pai/Guardião ou Mãe/Guardiã”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSGjriNvCikF"
      },
      "source": [
        "data_df['q33'] = data_df['q33'].apply(lambda x: x - 1 if x >= 3 else x)\n",
        "data_df = binarize_categorical_columns(data_df, ['q33'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbraLqD9NM7c"
      },
      "source": [
        "---\n",
        "\n",
        "### Padronização dos dados "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gecuinlDNMcV"
      },
      "source": [
        "y = data_df['q24']\n",
        "X = data_df.drop(['q24'], axis=1)\n",
        "\n",
        "ss = StandardScaler()\n",
        "new_values = ss.fit_transform(X)\n",
        "new_values_df = pd.DataFrame(new_values)\n",
        "new_values_df.columns = X.columns\n",
        "\n",
        "data_df = pd.concat([y, new_values_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IBNCARTMT2U"
      },
      "source": [
        "---\n",
        "\n",
        "### Visualização dos dados transformados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "sU1YDKCeMY6W",
        "outputId": "5cb375b2-7e9e-4824-b4f5-441fb49e6342"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>q24</th>\n",
              "      <th>q1</th>\n",
              "      <th>q2</th>\n",
              "      <th>q3</th>\n",
              "      <th>q4</th>\n",
              "      <th>q5</th>\n",
              "      <th>q6</th>\n",
              "      <th>q10</th>\n",
              "      <th>q15</th>\n",
              "      <th>q16</th>\n",
              "      <th>q17</th>\n",
              "      <th>q22</th>\n",
              "      <th>q23</th>\n",
              "      <th>q27</th>\n",
              "      <th>q28</th>\n",
              "      <th>q29</th>\n",
              "      <th>q31</th>\n",
              "      <th>q32</th>\n",
              "      <th>q34</th>\n",
              "      <th>q35</th>\n",
              "      <th>q36</th>\n",
              "      <th>q38</th>\n",
              "      <th>q39</th>\n",
              "      <th>q40</th>\n",
              "      <th>q41</th>\n",
              "      <th>q42</th>\n",
              "      <th>q43</th>\n",
              "      <th>q44</th>\n",
              "      <th>q45</th>\n",
              "      <th>q46</th>\n",
              "      <th>q49</th>\n",
              "      <th>q50</th>\n",
              "      <th>q51</th>\n",
              "      <th>q52</th>\n",
              "      <th>q53</th>\n",
              "      <th>q54</th>\n",
              "      <th>q55</th>\n",
              "      <th>q56</th>\n",
              "      <th>q57</th>\n",
              "      <th>q58</th>\n",
              "      <th>q61</th>\n",
              "      <th>q62</th>\n",
              "      <th>q63</th>\n",
              "      <th>q66</th>\n",
              "      <th>q67</th>\n",
              "      <th>q68</th>\n",
              "      <th>q33_1</th>\n",
              "      <th>q33_2</th>\n",
              "      <th>q33_3</th>\n",
              "      <th>q33_4</th>\n",
              "      <th>q18_1</th>\n",
              "      <th>q18_2</th>\n",
              "      <th>q18_3</th>\n",
              "      <th>q18_4</th>\n",
              "      <th>q18_5</th>\n",
              "      <th>q18_6</th>\n",
              "      <th>q18_7</th>\n",
              "      <th>q18_8</th>\n",
              "      <th>q37_1</th>\n",
              "      <th>q37_2</th>\n",
              "      <th>q37_3</th>\n",
              "      <th>q37_4</th>\n",
              "      <th>q37_5</th>\n",
              "      <th>q37_6</th>\n",
              "      <th>q37_7</th>\n",
              "      <th>q47_1</th>\n",
              "      <th>q47_2</th>\n",
              "      <th>q47_3</th>\n",
              "      <th>q48_1</th>\n",
              "      <th>q48_2</th>\n",
              "      <th>q48_3</th>\n",
              "      <th>q48_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-1.404354</td>\n",
              "      <td>-0.951660</td>\n",
              "      <td>-0.822009</td>\n",
              "      <td>-0.341102</td>\n",
              "      <td>-0.011165</td>\n",
              "      <td>0.722481</td>\n",
              "      <td>-0.549264</td>\n",
              "      <td>-0.330885</td>\n",
              "      <td>-0.405095</td>\n",
              "      <td>-0.513723</td>\n",
              "      <td>2.250050</td>\n",
              "      <td>2.513556</td>\n",
              "      <td>-0.583159</td>\n",
              "      <td>2.600614</td>\n",
              "      <td>4.189467</td>\n",
              "      <td>2.514529</td>\n",
              "      <td>1.703890</td>\n",
              "      <td>1.957162</td>\n",
              "      <td>3.482920</td>\n",
              "      <td>1.674510</td>\n",
              "      <td>-0.649130</td>\n",
              "      <td>5.141666</td>\n",
              "      <td>-0.380805</td>\n",
              "      <td>-0.339679</td>\n",
              "      <td>-0.266606</td>\n",
              "      <td>-0.131817</td>\n",
              "      <td>1.202517</td>\n",
              "      <td>2.615112</td>\n",
              "      <td>2.953849</td>\n",
              "      <td>1.642129</td>\n",
              "      <td>1.258124</td>\n",
              "      <td>-0.345366</td>\n",
              "      <td>2.139845</td>\n",
              "      <td>3.866711</td>\n",
              "      <td>-1.918971</td>\n",
              "      <td>-1.096169</td>\n",
              "      <td>1.258969</td>\n",
              "      <td>1.070513</td>\n",
              "      <td>-0.783117</td>\n",
              "      <td>-1.147324</td>\n",
              "      <td>-1.427522</td>\n",
              "      <td>1.605918</td>\n",
              "      <td>1.948103</td>\n",
              "      <td>1.920009</td>\n",
              "      <td>-0.521026</td>\n",
              "      <td>-1.341018</td>\n",
              "      <td>1.785560</td>\n",
              "      <td>-0.305374</td>\n",
              "      <td>-0.185578</td>\n",
              "      <td>0.579260</td>\n",
              "      <td>-0.258486</td>\n",
              "      <td>-0.122996</td>\n",
              "      <td>-0.137486</td>\n",
              "      <td>-0.035241</td>\n",
              "      <td>-0.075464</td>\n",
              "      <td>-0.029692</td>\n",
              "      <td>-0.415745</td>\n",
              "      <td>-0.951728</td>\n",
              "      <td>-0.566389</td>\n",
              "      <td>5.940288</td>\n",
              "      <td>-0.387641</td>\n",
              "      <td>-0.257931</td>\n",
              "      <td>-0.05023</td>\n",
              "      <td>-0.249828</td>\n",
              "      <td>-1.321303</td>\n",
              "      <td>-0.626228</td>\n",
              "      <td>3.334971</td>\n",
              "      <td>-1.325519</td>\n",
              "      <td>-0.52866</td>\n",
              "      <td>-0.331907</td>\n",
              "      <td>4.604120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.404354</td>\n",
              "      <td>-0.951660</td>\n",
              "      <td>-0.822009</td>\n",
              "      <td>-0.341102</td>\n",
              "      <td>-0.011165</td>\n",
              "      <td>-0.597870</td>\n",
              "      <td>-0.549264</td>\n",
              "      <td>-0.330885</td>\n",
              "      <td>-0.405095</td>\n",
              "      <td>-0.513723</td>\n",
              "      <td>-1.116522</td>\n",
              "      <td>-1.175953</td>\n",
              "      <td>0.569197</td>\n",
              "      <td>-0.745956</td>\n",
              "      <td>-0.389541</td>\n",
              "      <td>-0.608264</td>\n",
              "      <td>-0.319737</td>\n",
              "      <td>-1.588679</td>\n",
              "      <td>-0.772189</td>\n",
              "      <td>-0.882673</td>\n",
              "      <td>-0.649130</td>\n",
              "      <td>-0.337691</td>\n",
              "      <td>-0.380805</td>\n",
              "      <td>-0.339679</td>\n",
              "      <td>-0.266606</td>\n",
              "      <td>-0.131817</td>\n",
              "      <td>-0.831589</td>\n",
              "      <td>-0.695445</td>\n",
              "      <td>-0.574166</td>\n",
              "      <td>0.801793</td>\n",
              "      <td>-0.439064</td>\n",
              "      <td>-1.483722</td>\n",
              "      <td>-0.548229</td>\n",
              "      <td>-0.524511</td>\n",
              "      <td>-0.244970</td>\n",
              "      <td>0.316283</td>\n",
              "      <td>1.258969</td>\n",
              "      <td>1.070513</td>\n",
              "      <td>-0.783117</td>\n",
              "      <td>-0.569591</td>\n",
              "      <td>-0.206672</td>\n",
              "      <td>-1.088696</td>\n",
              "      <td>-0.513320</td>\n",
              "      <td>-0.520831</td>\n",
              "      <td>-0.521026</td>\n",
              "      <td>0.745702</td>\n",
              "      <td>-0.560048</td>\n",
              "      <td>-0.305374</td>\n",
              "      <td>-0.185578</td>\n",
              "      <td>0.579260</td>\n",
              "      <td>-0.258486</td>\n",
              "      <td>-0.122996</td>\n",
              "      <td>-0.137486</td>\n",
              "      <td>-0.035241</td>\n",
              "      <td>-0.075464</td>\n",
              "      <td>-0.029692</td>\n",
              "      <td>-0.415745</td>\n",
              "      <td>1.050720</td>\n",
              "      <td>-0.566389</td>\n",
              "      <td>-0.168342</td>\n",
              "      <td>-0.387641</td>\n",
              "      <td>-0.257931</td>\n",
              "      <td>-0.05023</td>\n",
              "      <td>-0.249828</td>\n",
              "      <td>0.756829</td>\n",
              "      <td>-0.626228</td>\n",
              "      <td>-0.299853</td>\n",
              "      <td>0.754421</td>\n",
              "      <td>-0.52866</td>\n",
              "      <td>-0.331907</td>\n",
              "      <td>-0.217197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.805733</td>\n",
              "      <td>1.050796</td>\n",
              "      <td>-0.822009</td>\n",
              "      <td>1.287714</td>\n",
              "      <td>0.842714</td>\n",
              "      <td>-0.597870</td>\n",
              "      <td>1.529177</td>\n",
              "      <td>-0.330885</td>\n",
              "      <td>1.273838</td>\n",
              "      <td>-0.513723</td>\n",
              "      <td>-1.116522</td>\n",
              "      <td>0.668801</td>\n",
              "      <td>0.569197</td>\n",
              "      <td>2.600614</td>\n",
              "      <td>-0.389541</td>\n",
              "      <td>1.473598</td>\n",
              "      <td>-0.319737</td>\n",
              "      <td>1.957162</td>\n",
              "      <td>-0.063004</td>\n",
              "      <td>0.395919</td>\n",
              "      <td>0.506643</td>\n",
              "      <td>-0.337691</td>\n",
              "      <td>-0.380805</td>\n",
              "      <td>-0.339679</td>\n",
              "      <td>-0.266606</td>\n",
              "      <td>-0.131817</td>\n",
              "      <td>1.202517</td>\n",
              "      <td>2.615112</td>\n",
              "      <td>0.013836</td>\n",
              "      <td>0.381624</td>\n",
              "      <td>-1.117939</td>\n",
              "      <td>1.362167</td>\n",
              "      <td>0.123790</td>\n",
              "      <td>-0.524511</td>\n",
              "      <td>-0.244970</td>\n",
              "      <td>-1.096169</td>\n",
              "      <td>1.258969</td>\n",
              "      <td>1.070513</td>\n",
              "      <td>-0.783117</td>\n",
              "      <td>0.585875</td>\n",
              "      <td>-1.427522</td>\n",
              "      <td>1.066995</td>\n",
              "      <td>-0.513320</td>\n",
              "      <td>-0.520831</td>\n",
              "      <td>-0.521026</td>\n",
              "      <td>0.745702</td>\n",
              "      <td>-0.560048</td>\n",
              "      <td>-0.305374</td>\n",
              "      <td>-0.185578</td>\n",
              "      <td>0.579260</td>\n",
              "      <td>-0.258486</td>\n",
              "      <td>-0.122996</td>\n",
              "      <td>-0.137486</td>\n",
              "      <td>-0.035241</td>\n",
              "      <td>-0.075464</td>\n",
              "      <td>-0.029692</td>\n",
              "      <td>-0.415745</td>\n",
              "      <td>-0.951728</td>\n",
              "      <td>-0.566389</td>\n",
              "      <td>5.940288</td>\n",
              "      <td>-0.387641</td>\n",
              "      <td>-0.257931</td>\n",
              "      <td>-0.05023</td>\n",
              "      <td>-0.249828</td>\n",
              "      <td>-1.321303</td>\n",
              "      <td>1.596862</td>\n",
              "      <td>-0.299853</td>\n",
              "      <td>-1.325519</td>\n",
              "      <td>-0.52866</td>\n",
              "      <td>-0.331907</td>\n",
              "      <td>4.604120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.404354</td>\n",
              "      <td>-0.951660</td>\n",
              "      <td>-0.822009</td>\n",
              "      <td>-0.341102</td>\n",
              "      <td>-0.011165</td>\n",
              "      <td>-0.597870</td>\n",
              "      <td>-0.549264</td>\n",
              "      <td>-0.330885</td>\n",
              "      <td>-0.405095</td>\n",
              "      <td>-0.513723</td>\n",
              "      <td>-0.274879</td>\n",
              "      <td>0.668801</td>\n",
              "      <td>0.569197</td>\n",
              "      <td>-0.745956</td>\n",
              "      <td>-0.389541</td>\n",
              "      <td>-0.608264</td>\n",
              "      <td>-0.994279</td>\n",
              "      <td>-1.588679</td>\n",
              "      <td>-0.772189</td>\n",
              "      <td>-0.882673</td>\n",
              "      <td>-0.649130</td>\n",
              "      <td>-0.337691</td>\n",
              "      <td>-0.380805</td>\n",
              "      <td>-0.339679</td>\n",
              "      <td>-0.266606</td>\n",
              "      <td>-0.131817</td>\n",
              "      <td>-0.831589</td>\n",
              "      <td>-0.695445</td>\n",
              "      <td>-0.574166</td>\n",
              "      <td>-0.458712</td>\n",
              "      <td>-0.439064</td>\n",
              "      <td>-0.345366</td>\n",
              "      <td>-1.220247</td>\n",
              "      <td>-0.524511</td>\n",
              "      <td>0.592030</td>\n",
              "      <td>1.022509</td>\n",
              "      <td>0.593917</td>\n",
              "      <td>1.070513</td>\n",
              "      <td>-0.783117</td>\n",
              "      <td>-1.147324</td>\n",
              "      <td>-0.817097</td>\n",
              "      <td>-0.549773</td>\n",
              "      <td>-0.513320</td>\n",
              "      <td>-0.520831</td>\n",
              "      <td>-0.521026</td>\n",
              "      <td>0.745702</td>\n",
              "      <td>-0.560048</td>\n",
              "      <td>-0.305374</td>\n",
              "      <td>-0.185578</td>\n",
              "      <td>0.579260</td>\n",
              "      <td>-0.258486</td>\n",
              "      <td>-0.122996</td>\n",
              "      <td>-0.137486</td>\n",
              "      <td>-0.035241</td>\n",
              "      <td>-0.075464</td>\n",
              "      <td>-0.029692</td>\n",
              "      <td>-0.415745</td>\n",
              "      <td>1.050720</td>\n",
              "      <td>-0.566389</td>\n",
              "      <td>-0.168342</td>\n",
              "      <td>-0.387641</td>\n",
              "      <td>-0.257931</td>\n",
              "      <td>-0.05023</td>\n",
              "      <td>-0.249828</td>\n",
              "      <td>0.756829</td>\n",
              "      <td>-0.626228</td>\n",
              "      <td>-0.299853</td>\n",
              "      <td>0.754421</td>\n",
              "      <td>-0.52866</td>\n",
              "      <td>-0.331907</td>\n",
              "      <td>-0.217197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-1.404354</td>\n",
              "      <td>1.050796</td>\n",
              "      <td>-0.822009</td>\n",
              "      <td>1.162420</td>\n",
              "      <td>-1.035821</td>\n",
              "      <td>-0.597870</td>\n",
              "      <td>0.836363</td>\n",
              "      <td>-0.330885</td>\n",
              "      <td>0.434371</td>\n",
              "      <td>0.437937</td>\n",
              "      <td>-1.116522</td>\n",
              "      <td>-0.253576</td>\n",
              "      <td>0.569197</td>\n",
              "      <td>-0.745956</td>\n",
              "      <td>-0.389541</td>\n",
              "      <td>-0.608264</td>\n",
              "      <td>-0.994279</td>\n",
              "      <td>0.437516</td>\n",
              "      <td>-0.063004</td>\n",
              "      <td>-0.030278</td>\n",
              "      <td>-0.649130</td>\n",
              "      <td>-0.337691</td>\n",
              "      <td>-0.380805</td>\n",
              "      <td>-0.339679</td>\n",
              "      <td>-0.266606</td>\n",
              "      <td>-0.131817</td>\n",
              "      <td>-0.831589</td>\n",
              "      <td>-0.695445</td>\n",
              "      <td>-0.574166</td>\n",
              "      <td>-0.458712</td>\n",
              "      <td>-0.439064</td>\n",
              "      <td>-0.345366</td>\n",
              "      <td>-1.220247</td>\n",
              "      <td>-0.524511</td>\n",
              "      <td>0.592030</td>\n",
              "      <td>1.728735</td>\n",
              "      <td>1.258969</td>\n",
              "      <td>1.070513</td>\n",
              "      <td>-0.783117</td>\n",
              "      <td>1.741341</td>\n",
              "      <td>-0.817097</td>\n",
              "      <td>-0.010851</td>\n",
              "      <td>-0.513320</td>\n",
              "      <td>-0.520831</td>\n",
              "      <td>-0.521026</td>\n",
              "      <td>0.745702</td>\n",
              "      <td>-0.560048</td>\n",
              "      <td>-0.305374</td>\n",
              "      <td>-0.185578</td>\n",
              "      <td>-1.726341</td>\n",
              "      <td>3.868683</td>\n",
              "      <td>-0.122996</td>\n",
              "      <td>-0.137486</td>\n",
              "      <td>-0.035241</td>\n",
              "      <td>-0.075464</td>\n",
              "      <td>-0.029692</td>\n",
              "      <td>-0.415745</td>\n",
              "      <td>-0.951728</td>\n",
              "      <td>-0.566389</td>\n",
              "      <td>-0.168342</td>\n",
              "      <td>-0.387641</td>\n",
              "      <td>-0.257931</td>\n",
              "      <td>-0.05023</td>\n",
              "      <td>4.002753</td>\n",
              "      <td>0.756829</td>\n",
              "      <td>-0.626228</td>\n",
              "      <td>-0.299853</td>\n",
              "      <td>0.754421</td>\n",
              "      <td>-0.52866</td>\n",
              "      <td>-0.331907</td>\n",
              "      <td>-0.217197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   q24        q1        q2        q3  ...     q48_1    q48_2     q48_3     q48_4\n",
              "0    1 -1.404354 -0.951660 -0.822009  ... -1.325519 -0.52866 -0.331907  4.604120\n",
              "1    0 -1.404354 -0.951660 -0.822009  ...  0.754421 -0.52866 -0.331907 -0.217197\n",
              "2    0  0.805733  1.050796 -0.822009  ... -1.325519 -0.52866 -0.331907  4.604120\n",
              "3    0 -1.404354 -0.951660 -0.822009  ...  0.754421 -0.52866 -0.331907 -0.217197\n",
              "4    0 -1.404354  1.050796 -0.822009  ...  0.754421 -0.52866 -0.331907 -0.217197\n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsAK9jqOZzw3",
        "outputId": "b5a81056-a92e-4523-fbfe-c13dbe2e1b78"
      },
      "source": [
        "print(\"Total de atributos:\", data_df.shape[1], \"\\nTotal de exemplos: \", data_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de atributos: 72 \n",
            "Total de exemplos:  55628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppGF5DMgOrG6"
      },
      "source": [
        "## Redução de dimensão\n",
        "\n",
        "Com a grande quantidade de colunas geradas na etapa de transformação dos dados e mostrado acima, se torna necessária a redução dessa dimensionalidade, visando otimização de performance do treinamento dos modelos.\n",
        "\n",
        "Vamos primeiro realizar uma análise dos principais componentes entre os atributos. Para isso será utilizado o modulo `PCA` da bibliotaca `scikit learn` e será analisado a explicabilidade dos primeiros 15 atributos mais importantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "oYfwGI5rbKwC",
        "outputId": "829d2c79-81f5-4174-bc19-684d70826d08"
      },
      "source": [
        "X = data_df.drop(['q24'], axis=1)\n",
        "y = data_df['q24']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=10, stratify=y)\n",
        "\n",
        "pca = PCA(n_components=None)\n",
        "\n",
        "pca.fit(X_train)\n",
        "\n",
        "pca_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "pca_ratio_slice = pca_ratio[:15]\n",
        "cum_ratio = np.cumsum(pca_ratio_slice)\n",
        "\n",
        "plt.bar(range(len(pca_ratio_slice)), pca_ratio_slice, alpha=0.5, align='center', label='variância explicada - individual')\n",
        "plt.step(range(len(cum_ratio)), cum_ratio, where='mid', label='variância explicada - cumulativa')\n",
        "\n",
        "plt.ylabel('Taxa de variância explicada')\n",
        "plt.xlabel('Componentes principais')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lTVbJ5fcXJJ"
      },
      "source": [
        "Com o gráfico acima é possivel notar que a técnica `PCA` não se saiu bem neste dataset, pois, nenhum pequeno grupo de atributos responde à uma boa taxa de explicabilidade dos dados.\n",
        "\n",
        "A partir dessa conclusão poderíamos partir para utilização da técnica `LDA`, porém, essa técnica seleciona `c - 1` atributos, onde `c` é o número de classes. Como há somente duas classes essa técnica é inútil para o problema, então, para utilizar uma técnica supervisionada pode-se treinar uma RandomForest para analisar a importância dos atributos, como a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYHs7FbGh24_",
        "outputId": "4e1a241f-c281-4df8-a020-47fd84bc8cfe"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=1000,\n",
        "                            random_state=10,\n",
        "                            n_jobs=-1)\n",
        "\n",
        "rf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=10)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgl3auMYmQOm"
      },
      "source": [
        "Com a floresta treinada podemos olhar para importância de cada atributos, como segue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "4kQAQT0LmXk5",
        "outputId": "f0553a1e-7117-4aeb-f8e1-4af85ae87326"
      },
      "source": [
        "importances = rf.feature_importances_\n",
        "\n",
        "indices = np.argsort(importances)[::-1]\n",
        "indices_slice = indices[:15]\n",
        "\n",
        "plt.title('Importância dos atributos')\n",
        "plt.bar(range(15), importances[indices_slice], color='lightblue', align='center')\n",
        "plt.xticks(range(15), X_train.columns[indices_slice], rotation=90)\n",
        "plt.xlim([-1, 15])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZTqxNC38Vf"
      },
      "source": [
        "Agora, com o grau de importância dos atributos é possível notar que o padrão de pouca variação de explicabilidade dos atributos vista na aplicação do `PCA` continua. Então, serão utilizadas os atributos que correspondem à até 70% de importância."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvxKr8Nd4v0V",
        "outputId": "5c24182a-e951-4aee-cc9e-1385d40a9c8c"
      },
      "source": [
        "selected_indices = [idx for idx, cumsum in zip(indices, np.cumsum(importances[indices])) if cumsum <= 0.7]\n",
        "selected_columns = X_train.columns[selected_indices].tolist()\n",
        "\n",
        "print('Atributos selecionados:', selected_columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atributos selecionados: ['q22', 'q23', 'q56', 'q5', 'q4', 'q49', 'q62', 'q52', 'q61', 'q57', 'q63', 'q58', 'q1', 'q54', 'q34', 'q3', 'q28', 'q50', 'q15', 'q55', 'q51', 'q32', 'q68', 'q2', 'q36', 'q10']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaUYSut8DyE"
      },
      "source": [
        "Com a lista de atributos selecionados é possível finaliar a transformação do dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppWbHh8i8R0n"
      },
      "source": [
        "X_train = X_train[selected_columns]\n",
        "X_test = X_test[selected_columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZS3AYpaD_AN"
      },
      "source": [
        "## Treinamento e avaliação dos modelos\n",
        "\n",
        "O dicionário abaixo será utilizado para agrupar os modelos treinados e suas respectivas métricas e a variável `n_folds` corresponde a quantidade de paginações que serão executadas no treinamento e teste de cada modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_2MF8hUEEOE"
      },
      "source": [
        "trained_models = {}\n",
        "n_folds = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CjIfkqeHENS"
      },
      "source": [
        "---\n",
        "\n",
        "### Treinamento KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzHKgFb8HYy3",
        "outputId": "68482605-22a1-4551-ae77-187f4fb4b82c"
      },
      "source": [
        "knn = KNeighborsClassifier()\n",
        "\n",
        "knn_grid = {\n",
        "    'n_neighbors': [3, 5, 7],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "\n",
        "knn_gscv = GridSearchCV(estimator=knn,\n",
        "                        param_grid=knn_grid,\n",
        "                        scoring='f1',\n",
        "                        cv=n_folds)\n",
        "\n",
        "knn_gscv.fit(X_train, y_train)\n",
        "\n",
        "trained_models['KNN'] = {'score': knn_gscv.best_score_,\n",
        "                         'estimator': knn_gscv}\n",
        "print(f\"KNN F1 Score: {trained_models['KNN']['score']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN F1 Score: 0.40966786840297936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6l6mqe5RPM5"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Treinamento SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZXuw1ZtM_lT",
        "outputId": "187a2cfb-84ed-4cae-a923-d41ea13d8931"
      },
      "source": [
        "svc = SVC()\n",
        "\n",
        "svm_grid = {\n",
        "    'C': [.5, 1.0, 1.5],\n",
        "    'kernel': ['poly', 'rbf']\n",
        "}\n",
        "\n",
        "svm_gscv = GridSearchCV(estimator=svc,\n",
        "                        param_grid=svm_grid,\n",
        "                        scoring='f1',\n",
        "                        cv=n_folds)\n",
        "\n",
        "svm_gscv.fit(X_train, y_train)\n",
        "\n",
        "trained_models['SVM'] = {'score': svm_gscv.best_score_,\n",
        "                         'estimator': svm_gscv}\n",
        "print(f\"SVM F1 Score: {trained_models['SVM']['score']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM F1 Score: 0.44033881271641473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQlJDvoyRKqj"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Treinamento XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtKF6ENDRUqE",
        "outputId": "8815b2d5-4213-424c-c0c4-1727ad523d1f"
      },
      "source": [
        "xgb = XGBClassifier()\n",
        "\n",
        "xgb_grid = {\n",
        "    'objective':['binary:logistic', 'binary:hinge'],\n",
        "    'learning_rate': [.05, .3],\n",
        "    'max_depth': [5, 10],\n",
        "    'subsample': [0.5, 1],\n",
        "    'colsample_bytree': [0.5, 1],\n",
        "    'n_estimators': [100, 500],\n",
        "    'use_label_encoder': [False],\n",
        "    'verbosity': [0]\n",
        "}\n",
        "\n",
        "xgb_gscv = GridSearchCV(estimator=xgb,\n",
        "                        param_grid=xgb_grid,\n",
        "                        scoring='f1',\n",
        "                        cv=n_folds) \n",
        "\n",
        "xgb_gscv.fit(X_train, y_train)\n",
        "\n",
        "trained_models['XGB'] = {'score': xgb_gscv.best_score_,\n",
        "                         'estimator': xgb_gscv}\n",
        "print(f\"XGB F1 Score: {trained_models['XGB']['score']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB F1 Score: 0.5066599292159832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647V1NyeXOBK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secOfgauXQw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e999c528-e86b-47a2-d96f-2e4cb5500b21"
      },
      "source": [
        "for model in trained_models.keys():\n",
        "    estimator = trained_models[model]['estimator']\n",
        "    test_score = round(trained_models[model][\"score\"], 2)\n",
        "    y_pred = estimator.predict(X_test)\n",
        "    evaluation_score = round(f1_score(y_test, y_pred), 2)\n",
        "    print(f'{model}: Test: {test_score} - True: {evaluation_score}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN: Test: 0.41 - True: 0.41\n",
            "SVM: Test: 0.44 - True: 0.44\n",
            "XGB: Test: 0.51 - True: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-svzT-fkmU8"
      },
      "source": [
        "Como está sendo utilizado a métrica F1, o melhor resultado possível seria `1.0`, porém, com a célula acima, é possível notar que os modelos não obtiveram um resultado satisfatório."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppimel9Cvc3G"
      },
      "source": [
        "\n",
        "## Conclusão\n",
        "Com o baixo desempenho dos modelos explicitado acima pode-se considerar duas possibilidades. Primeira, que os dados selecionados não foram bem trabalhados, ou seja, não foi realizada uma boa engenharia de atributos. E, segunda, os dados não possuem uma boa separabilidade, isso é, as classes não estão bem definidas, dificultado assim a decibilidade dos modelos treinados.\n",
        "\n",
        "Infelizmente não houve tempo de testar mais modelos e outras técnicas de transformação dos dados, porém, não vejo muitas esperanças. Atribuo o fato do baixo desempenho dos modelos à baixa separabilidade das classes utilizadas. Pois, como visto na seção de redução de dimensionalidade, não haviam atributos explicativos o suficiente para optar pela técnica PCA."
      ]
    }
  ]
}